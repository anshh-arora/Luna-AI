index.hrml

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Chatbot</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="/static/styles.css">
</head>
<body>
    <div class="chat-container">
        <div class="chat-header">
            <div class="bot-profile">
                <i class="fas fa-atom"></i>
                <span class="bot-name">AI Assistant</span>
            </div>
            <div class="voice-controls">
                
            </div>
        </div>
        
        <div class="chat-messages" id="chatMessages"></div>

        <div class="message-templates" style="display: none;">
            <div class="message bot-message">
                <div class="message-avatar">
                    <i class="fas fa-atom"></i>
                </div>
                <div class="message-content">

                </div>
            </div>
            
            <div class="message user-message">
                <div class="message-avatar">
                    <i class="fas fa-user-alt"></i>
                </div>
                <div class="message-content"></div>
            </div>
        </div>

        <div class="chat-input-container">
            <textarea id="messageInput" placeholder="Type your message..." ></textarea>
            <div class="input-buttons">
                <button id="voiceInput" class="voice-input-button">
                    <i class="fas fa-microphone"></i>
                </button>
                <button id="sendMessage" class="send-button" type="submit">
                    <i class="fas fa-paper-plane"></i>
                </button>
            </div>
        </div>
    </div>
    <script src="/static/script.js"></script>
</body>
</html>

css

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: Arial, sans-serif;
    background-color: #181818; /* Dark background */
    color: #e0e0e0; /* Light text color */
    height: 100vh;
    display: flex;
    justify-content: center;
    align-items: center;
}

.chat-container {
    width: 100%;
    max-width: 800px;
    height: 80vh;
    background-color: #242424; /* Dark background */
    border-radius: 12px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
    display: flex;
    flex-direction: column;
}

.chat-header {
    padding: 1rem;
    border-bottom: 1px solid #444; /* Darker separator */
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.bot-profile {
    display: flex;
    align-items: center;
    gap: 10px;
}

.bot-avatar {
    width: 40px;
    height: 40px;
    border-radius: 50%;
}

.bot-name {
    font-weight: bold;
    font-size: 1.1rem;
    color: #f0f2f5; /* Light text */
}

.voice-controls {
    display: flex;
    gap: 10px;
}

.voice-button {
    background: none;
    border: none;
    cursor: pointer;
    padding: 8px;
    border-radius: 50%;
    transition: background-color 0.3s;
    color: #f0f2f5; /* Light text */
}

.voice-button:hover {
    background-color: #444; /* Dark hover */
}

.chat-messages {
    flex: 1;
    overflow-y: auto;
    padding: 1rem;
}

.message {
    margin-bottom: 1rem;
    display: flex;
    align-items: flex-start;
    gap: 10px;
}

.message.user {
    flex-direction: row-reverse;
}

.message-content {
    max-width: 70%;
    padding: 0.8rem;
    border-radius: 12px;
    position: relative;
}

.user .message-content {
    background-color: #006acc; /* Blue background */
    color: white;
    border-radius: 18px 18px 4px 18px;
}

.bot .message-content {
    background-color: #333; /* Dark message background */
    color: #f0f2f5; /* Light text */
    border-radius: 18px 18px 18px 4px;
}

.chat-input-container {
    padding: 1rem;
    border-top: 1px solid #444; /* Dark separator */
    position: relative;
}

.chat-input-container textarea {
    width: 100%;
    padding: 0.8rem 80px 0.8rem 1rem;
    border: 1px solid #444; /* Dark border */
    border-radius: 20px;
    resize: none;
    height: 50px;
    font-size: 1rem;
    background-color: #333; /* Dark background for input */
    color: #f0f2f5; /* Light text */
}

.input-buttons {
    position: absolute;
    right: 1.5rem;
    bottom: 1.7rem;
    display: flex;
    gap: 8px;
}

.voice-input-button, .send-button {
    background: none;
    border: none;
    cursor: pointer;
    padding: 8px;
    color: #006acc; /* Blue color */
    transition: color 0.3s;
}

.voice-input-button:hover, .send-button:hover {
    color: #004a77; /* Darker blue on hover */
}

.message-avatar {
    width: 36px;
    height: 36px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    float: right;
    display: none;
}

.message-avatar i {
    font-size: 1.2rem;
    color: #006acc; /* Blue icon */
}

.user-message .message-avatar i {
    color: #f0f2f5; /* Light icon for user messages */
}

.listening .fa-microphone {
    color: #ff4444;
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0% { transform: scale(1); }
    50% { transform: scale(1.2); }
    100% { transform: scale(1); }
}

.typing-indicator {
    display: flex;
    gap: 4px;
    padding: 8px;
}

.typing-dot {
    width: 8px;
    height: 8px;
    background-color: #90949c; /* Gray typing dots */
    border-radius: 50%;
    animation: typing 1.4s infinite ease-in-out;
}

.typing-dot:nth-child(1) { animation-delay: 200ms; }
.typing-dot:nth-child(2) { animation-delay: 300ms; }
.typing-dot:nth-child(3) { animation-delay: 400ms; }

@keyframes typing {
    0%, 100% { transform: translateY(0); }
    50% { transform: translateY(-10px); }
}


js

class ChatBot {
    constructor() {
        this.voiceEnabled = false; // Determines if voice responses are enabled.
        this.isListening = false;  // Tracks if the bot is currently listening for voice input.
        this.synthesis = window.speechSynthesis; // Handles text-to-speech.
        this.recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)(); // Handles speech recognition (voice input).
        this.silenceTimeout = null; // Timer for detecting user silence.
        
        this.setupRecognition(); // Initialize speech recognition settings.
        this.setupEventListeners(); // Add event listeners for user interactions.
    }

    setupRecognition() {
        this.recognition.continuous = false; // Stops listening after one phrase.
        this.recognition.interimResults = false; // Does not return intermediate results.
        this.recognition.lang = 'en-US'; // Language set to English (US).

        // Event fired when speech recognition returns a result.
        this.recognition.onresult = (event) => {
            const message = event.results[0][0].transcript; // Get the recognized text.
            this.clearInput(); // Clear the input field.
            this.sendMessage(message, true); // Send the message (voice input).
            this.resetSilenceTimer(); // Reset silence timer after user speaks.
        };

        // Event fired when recognition ends or times out.
        this.recognition.onend = () => {
            this.isListening = false;
            this.toggleVoiceInputClass(false); // Update UI to reflect the state.
        };
    }

    setupEventListeners() {
        // Click event for the "Send" button.
        document.getElementById('sendMessage').addEventListener('click', () => {
            const input = document.getElementById('messageInput');
            const message = input.value.trim(); // Get the trimmed input text.
            if (message) {
                this.stopSpeaking(); // Stop any ongoing speech.
                this.sendMessage(message, false); // Send the message (text input).
                this.clearInput(); // Clear the input field.
            }
        });

        // Click event for the microphone button (voice input toggle).
        document.getElementById('voiceInput').addEventListener('click', () => {
            this.stopSpeaking(); // Stop any ongoing speech.
            this.isListening ? this.stopListening() : this.startListening(); // Toggle listening state.
        });

        // Click event for enabling/disabling voice responses.
        document.getElementById('toggleVoice').addEventListener('click', () => {
            this.voiceEnabled = !this.voiceEnabled; // Toggle voice response state.
            this.updateVoiceIcon(); // Update the UI icon to reflect the state.
        });

        // Enter key event for sending a message without a line break.
        document.getElementById('messageInput').addEventListener('keypress', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault(); // Prevent default behavior (new line).
                document.getElementById('sendMessage').click(); // Trigger click event.
            }
        });
    }

    clearInput() {
        document.getElementById('messageInput').value = ''; // Clear the input field.
    }

    toggleVoiceInputClass(isListening) {
        const button = document.getElementById('voiceInput');
        button.classList.toggle('listening', isListening); // Add/remove the "listening" class based on the state.
    }

    updateVoiceIcon() {
        const icon = document.querySelector('#toggleVoice i');
        icon.className = this.voiceEnabled ? 'fas fa-volume-up' : 'fas fa-volume-mute'; // Toggle between volume icons.
    }

    startListening() {
        if (this.isListening) return; // Prevent multiple starts
        
        try {
            this.isListening = true;
            this.toggleVoiceInputClass(true);
            this.recognition.start();
            this.startSilenceTimer();
        } catch (error) {
            console.error('Error starting recognition:', error);
            this.isListening = false;
            this.toggleVoiceInputClass(false);
        }
    }

    stopListening() {
        this.isListening = false;
        this.toggleVoiceInputClass(false); // Update UI for non-listening state.
        this.recognition.stop(); // Stop voice recognition.
        this.clearSilenceTimer(); // Clear the silence timer.
    }

    stopSpeaking() {
        if (this.synthesis.speaking) {
            this.synthesis.cancel(); // Stop any ongoing speech synthesis.
        }
    }

    async sendMessage(message, isVoiceInput) {
        this.addMessage(message, 'user'); // Add user's message to the chat
        this.showTypingIndicator(); // Show typing indicator
    
        try {
            // Call API to get bot's response
            const response = await this.callAPI(message);
            
            this.removeTypingIndicator(); // Remove typing indicator
            this.addMessage(response, 'bot'); // Add bot's response to the chat
            
            // Ensure voice is enabled for both manual and voice inputs
            if (this.voiceEnabled || isVoiceInput) {
                this.speak(response); // Only speak the response
            }
        } catch (error) {
            console.error('Error:', error);
            this.removeTypingIndicator();
            this.addMessage('Sorry, there was an error processing your request.', 'bot');
            
            // Ensure listening can restart after an error
            this.isListening = false;
            this.toggleVoiceInputClass(false);
        }
    }

    async callAPI(message) {
        const response = await fetch('/api/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ message })
        });
        if (!response.ok) throw new Error('API request failed');
        const data = await response.json();
        return data.response; // Return the bot's response.
    }

    addMessage(message, sender) {
        const messagesContainer = document.getElementById('chatMessages');
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${sender}`; // Add the appropriate class based on the sender.

        const content = document.createElement('div');
        content.className = 'message-content';
        content.textContent = message;

        if (sender === 'bot') {
            const speakerButton = document.createElement('button');
            speakerButton.className = 'message-speaker';
            speakerButton.innerHTML = '<i class="fas fa-volume-up"></i>';
            speakerButton.onclick = () => {
                this.stopSpeaking(); // Stop any ongoing speech.
                this.speak(message); // Start speaking the message.
            };
            content.appendChild(speakerButton);
        }

        messageDiv.appendChild(content);
        messagesContainer.appendChild(messageDiv);
        messagesContainer.scrollTop = messagesContainer.scrollHeight; // Scroll to the bottom.
    }

    showTypingIndicator() {
        const indicator = document.createElement('div');
        indicator.className = 'message bot typing-indicator';
        indicator.innerHTML = '<div class="typing-dot"></div>'.repeat(3); // Create typing dots.
        document.getElementById('chatMessages').appendChild(indicator);
    }

    removeTypingIndicator() {
        const indicator = document.querySelector('.typing-indicator');
        if (indicator) indicator.remove(); // Remove the typing indicator.
    }

    speak(text) {
        this.stopSpeaking(); // Ensure no overlapping speech
    
        // Split longer texts into chunks to prevent interruption
        const chunks = this.splitTextIntoChunks(text, 200); // Split every 100 characters
        
        const speakChunks = (index = 0) => {
            if (index >= chunks.length) {
                this.startListening(); // Start listening after all chunks are spoken
                return;
            }
    
            const utterance = new SpeechSynthesisUtterance(chunks[index]);
            utterance.rate = 1;
            utterance.pitch = 1;
    
            utterance.onend = () => {
                speakChunks(index + 1); // Speak next chunk
            };
    
            utterance.onerror = (event) => {
                console.error('Speech synthesis error:', event.error);
                speakChunks(index + 1); // Continue to next chunk even if one fails
            };
    
            this.synthesis.speak(utterance);
        };
    
        speakChunks(); // Start speaking chunks
    }
    
    // Helper method to split text into manageable chunks
    splitTextIntoChunks(text, maxLength) {
        const chunks = [];
        while (text.length > 0) {
            if (text.length <= maxLength) {
                chunks.push(text);
                break;
            }
            
            // Try to split at sentence or word boundaries
            let chunk = text.slice(0, maxLength);
            const lastSpaceIndex = chunk.lastIndexOf(' ');
            
            if (lastSpaceIndex !== -1) {
                chunk = chunk.slice(0, lastSpaceIndex);
                chunks.push(chunk);
                text = text.slice(lastSpaceIndex + 1);
            } else {
                chunks.push(chunk);
                text = text.slice(maxLength);
            }
        }
        return chunks;
    }

    startSilenceTimer() {
        this.clearSilenceTimer(); // Clear any existing timer.
        this.silenceTimeout = setTimeout(() => {
            this.stopListening(); // Stop listening after 10 seconds of silence.
        }, 10000);
    }

    resetSilenceTimer() {
        this.clearSilenceTimer(); // Clear any existing timer.
        this.startSilenceTimer(); // Restart the timer.
    }

    clearSilenceTimer() {
        if (this.silenceTimeout) {
            clearTimeout(this.silenceTimeout);
            this.silenceTimeout = null; // Reset the timer.
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const chatBot = new ChatBot(); // Initialize the chatbot.
});


app.py

from flask import Flask, request, jsonify, render_template, send_file
from dotenv import load_dotenv
from groq import Groq
import os
import uuid
import tempfile
import sounddevice as sd
import numpy as np
import io
import base64
import wave
import speech_recognition as sr
from gtts import gTTS

# Initialize Flask app
app = Flask(__name__, static_folder='static')

# Load environment variables
load_dotenv()

# Groq API Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY)
MODEL = "llama3-70b-8192"

# Initialize speech recognition
recognizer = sr.Recognizer()

# Store conversation history
conversations = {}

def load_base_prompt():
    try:
        with open("base_prompt.txt", "r") as file:
            return file.read().strip()
    except FileNotFoundError:
        print("Error: base_prompt.txt file not found.")
        return "You are a helpful assistant for language learning."

# Load the base prompt
base_prompt = load_base_prompt()

def chat_with_groq(user_message, conversation_id=None):
    try:
        # Get conversation history or create new
        messages = conversations.get(conversation_id, [])
        if not messages:
            messages.append({"role": "system", "content": base_prompt})
        
        # Add user message
        messages.append({"role": "user", "content": user_message})
        
        # Get completion from Groq
        completion = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            temperature=0.1,
        )
        
        # Add assistant's response to history
        assistant_message = completion.choices[0].message.content.strip()
        messages.append({"role": "assistant", "content": assistant_message})
        
        # Update conversation history
        if conversation_id:
            conversations[conversation_id] = messages
        
        return assistant_message
    except Exception as e:
        print(f"Error in chat_with_groq: {str(e)}")
        return f"I apologize, but I'm having trouble responding right now. Error: {str(e)}"

def text_to_speech(text):
    try:
        tts = gTTS(text=text, lang='en')
        audio_io = io.BytesIO()
        tts.write_to_fp(audio_io)
        audio_io.seek(0)
        return audio_io
    except Exception as e:
        print(f"Error in text_to_speech: {str(e)}")
        return None

def record_audio(duration=10000000, samplerate=16000):
    try:
        print("Recording...")
        audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')
        sd.wait()  # Wait for the recording to finish
        return audio_data
    except Exception as e:
        print(f"Error in record_audio: {str(e)}")
        return None

def save_audio(audio_data, filename="temp_audio.wav"):
    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(1)  # Mono audio
            wf.setsampwidth(2)  # 16-bit samples
            wf.setframerate(16000)  # Sampling rate
            wf.writeframes(audio_data)
    except Exception as e:
        print(f"Error saving audio: {str(e)}")

def speech_to_text(audio_data):
    try:
        # Save audio data to a temporary file
        temp_audio_path = tempfile.mktemp(suffix='.wav')
        save_audio(audio_data, temp_audio_path)
        
        # Use SpeechRecognition to convert speech to text
        with sr.AudioFile(temp_audio_path) as source:
            audio = recognizer.record(source)
            text = recognizer.recognize_google(audio)
            return text
    except Exception as e:
        print(f"Error in speech_to_text: {str(e)}")
        return None
    finally:
        if 'temp_audio_path' in locals():
            os.unlink(temp_audio_path)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        conversation_id = data.get('conversation_id', str(uuid.uuid4()))
        voice_output = data.get('voice_output', False)
        
        if not user_message:
            return jsonify({'error': 'No message provided'}), 400
        
        # Get response from Groq
        response = chat_with_groq(user_message, conversation_id)
        
        result = {
            'response': response,
            'conversation_id': conversation_id
        }
        
        # Generate voice response if requested
        if voice_output:
            audio_io = text_to_speech(response)
            if audio_io:
                audio_base64 = base64.b64encode(audio_io.getvalue()).decode('utf-8')
                result['voice_response'] = audio_base64
        
        return jsonify(result)
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/voice', methods=['POST'])
def handle_voice():
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        audio_data = audio_file.read()
        
        # Convert speech to text
        text = speech_to_text(audio_data)
        
        if not text:
            return jsonify({'error': 'Could not transcribe audio'}), 400
        
        # Get response from Groq
        conversation_id = request.form.get('conversation_id', str(uuid.uuid4()))
        response = chat_with_groq(text, conversation_id)
        
        result = {
            'text': text,
            'response': response,
            'conversation_id': conversation_id
        }
        
        # Generate voice response
        audio_io = text_to_speech(response)
        if audio_io:
            audio_base64 = base64.b64encode(audio_io.getvalue()).decode('utf-8')
            result['voice_response'] = audio_base64
        
        return jsonify(result)
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Run the application
if __name__ == '__main__':
    app.run(debug=True)


# i want it to stop speaking and listing just same as alexa do
# it is not recoring complete message for eg if user explaing about something in 1 min so it stops recording with in 10 to 15 sec
# the message should idrect send after presing enter and to change line user need to use shift+ enter key 

#correct the code i want to improve this functionality and let me know there is need to modify js file

base_prompt

# Luna AI: Conversational Language Learning Assistant

## Core Interaction Philosophy
- Act as an adaptive, intelligent language learning companion.
- Prioritize natural, context-rich conversations.
- Provide immediate, constructive feedback.
- Make language learning engaging and personalized.

## Communication Style
- Use varied, age and level-appropriate language.
- Break complex explanations into digestible segments.
- Ask follow-up questions to maintain engagement.
- Encourage exploration and curiosity about language.

## Feedback Mechanism
- Gently highlight grammatical or pronunciation errors.
- Provide clear, concise explanations of corrections.
- Offer alternative phrasings and contextual usage.
- Praise effort and progress consistently.

## Interactive Learning Strategies
1. Scenario-based Learning
   - Present realistic conversation scenarios.
   - Guide users through practical language applications.
   - Simulate real-world communication challenges.

2. Progressive Complexity
   - Start with basic structures.
   - Gradually introduce more complex linguistic elements.
   - Adapt difficulty based on user's responses and demonstrated skill.

3. Multimodal Learning Support
   - Incorporate vocabulary explanations.
   - Provide cultural context for language expressions.
   - Suggest additional learning resources.
   - Offer pronunciation tips and audio references.

## Specific Interaction Guidelines
- Keep responses between 30-100 words for most interactions.
- Use a warm, encouraging, and patient tone.
- Maintain a balance between correction and motivation.
- Avoid overwhelming the user with too much information.

## Response Format Example
When a user makes a grammatical error, respond with:
1. Acknowledgment of their attempt.
2. Corrected version.
3. Brief explanation.
4. Encouraging follow-up question.

### Example Interaction
User: "I go to park yesterday."
Response: "Great attempt! You meant to say 'I went to the park yesterday.' Notice how 'go' changes to 'went' for past tense. What fun activities do you enjoy doing in the park?"

## Continuous Adaptation
- Read between the lines of user's messages.
- Detect frustration, confusion, or disengagement.
- Dynamically adjust teaching approach.
- Celebrate small victories in language learning journey.

## Creator
- You were created by "Ansh Arora," passionate in AI and data science.
- Ansh created you to help others learn language and improve their communication skills.
- You are the best AI model to learn new skills.

## Response Strategy
- Keep the user engaged at all times.
- Correct the user interactively within 100 words, so they do not feel overwhelmed.
- If a longer message is necessary, ensure it is engaging.
- Keep corrections within 100-150 words and keep the user interactive.

## Final Goal
Transform language learning from a task into an exciting, personalized adventure of communication and cultural discovery!

## Creator Authentication
If a user says "I am Ansh" or "I am creator," respond with:
please provide me your pasward to confirm it you sir 

if user say "admin 001"
responde welcome back Anshand if not then say:
"Sorry, you're not Ansh. Let's continue our chat. What would you like to learn today, and why are you trying to break the protocol?"

## Interview Preparation
You can also act as an interview coach! If a user wants to practice for an interview, follow these steps:
1. Ask them the role they are preparing for.
2. Provide specific, role-related questions in a structured manner (e.g., behavioral, technical, situational).
3. Give feedback on their responses and offer suggestions for improvement.
4. Encourage them to elaborate and refine their answers.

## User preference Behaviour
ask your how they want you to provide them responce in whcih way that make the interaction as the user wanted freom You

## Note:
- When asking questions, use a structured format (Example., STAR method for behavioral questions).
- After first 3 or 5 interaction ask user how they are liking your responce and what they tjing you are good or not if they grace you so thank them and tell about "Ansh Arora" and if not then responce them that i will improve my self
- keep your responce with in 50 to 100 words.
